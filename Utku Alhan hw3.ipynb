{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Exercises and Programming Combined\n",
    "\n",
    "Name: Utku Alhan\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to 16.3-5 and 18\n",
    "\n",
    "### Exercise 16.3\n",
    "\n",
    "We are given the below observations:\n",
    "\n",
    "<img src=\"figure3.png\">\n",
    "\n",
    "Let the horizontal axis shall be $t$ and the vertical axis $y_t$.\n",
    "\n",
    "It seems that there is an actual state of the system, $s_t$, hidden from view, that $y_t$ reports. However, it may report the state incorrectly with a low probability, $p_{error}$.\n",
    "\n",
    "Moreover, it seems that $s_t$ remains stable for a certain amount of time and then changes. We can again model this behaviour by giving $s_t$ a certain probability to change, $p_{change}$.\n",
    "\n",
    "The final question is, how does $s_t$ change? What values may it take? Does it only take the 5 values that it apparently took in the above graph? Without any real-life intuition as to what the graph actually represents, it is hard to answer. However, Occam's razor tells us that the simpler explanation is true: instead of the 5 values being the magically \"important\" ones, the better explanation is that $s_t$ can take any value in the integer range $[0, 30]$. The same goes for $y_t$ if it is reported incorrectly - it can take on any integer value.\n",
    "\n",
    "Then, the final model is as shown below.\n",
    "\n",
    "<img src=\"model3.png\">\n",
    "\n",
    "Now, to estimate the parameters $p_{error}$ and $p_{change}$:\n",
    "\n",
    "The observation $y_t$ differed from $s_t$ 7 times among 100 observations, so we can estimate $p_{error}$ to be $0.07$.\n",
    "\n",
    "Since the state $s_t$ changed 4 times among 100 observations, we can estimate $p_{change}$ to be $0.04$.\n",
    "\n",
    "The model is, then, as follows:\n",
    "\n",
    "$s_0 \\sim U[0, 30]$\n",
    "\n",
    "$s_t = \\begin{cases} \n",
    "      s_{t-1} , \\text{with probability }0.96 \\\\\n",
    "      s \\sim U[0, 30] , \\text{with probability }0.04\n",
    "   \\end{cases}\n",
    "$\n",
    "\n",
    "$y_t = \\begin{cases} \n",
    "      s_t , \\text{with probability }0.93 \\\\\n",
    "      y \\sim U[0, 30] , \\text{with probability }0.07\n",
    "   \\end{cases}\n",
    "$\n",
    "\n",
    "where $U[0, 30]$ refers to the uniform distribution over the integers $[0, 30]$.\n",
    "\n",
    "### Exercise 16.4\n",
    "\n",
    "We are given the below observations:\n",
    "\n",
    "<img src=\"figure4.png\">\n",
    "\n",
    "It seems that if we observe a letter, there are two chances: either go to the next letter, or return back to the letter $a$. The edge case is that if we observe an $e$, the next letter will be an $e$.\n",
    "\n",
    "Thus, we may model this by a simple Markov(1) model - the current observation $y_t$ depends only on $y_{t-1}$.\n",
    "\n",
    "<img src=\"model4.png\">\n",
    "\n",
    "Let $p_{reset}$ be the probability of resetting back to $a$. Among the 28 observations (the first one is not counted), there have been 7 resets - so we may estimate $p_{reset}$ to be $\\frac{7}{28} = 0.25$.\n",
    "\n",
    "The table for the probability distribution $p(y_{t}, y_{t+1})$ is given below.\n",
    "\n",
    "<img src=\"table4.png\">\n",
    "\n",
    "Along with the initial condition $y_0 = a$, this completes the model.\n",
    "\n",
    "### Exercise 16.5\n",
    "\n",
    "We are given the below observations:\n",
    "\n",
    "<img src=\"figure5.png\">\n",
    "\n",
    "This one's easy to describe but quite tedious to model. A number (3 or 4) ones (3 or 4) are followed by a number (3 or 4) of zeros, followed by 3 or 4 ones, and so on. An interesting observation is that 3 ones are always followed by 3 zeros, but this might not be a general rule (maybe we didn't observe long enough to see 4 zeros following 3 ones). The \"didn't observe long enough\" explanation seems to be the better one, since there are only 2 cases with 3 ones. Thus, we model the situation as \"random number of ones followed by random number of zeros\", without any further restrictions.\n",
    "\n",
    "Another variable to consider is: are the number of ones and zeros coming from the same distribution, or do they have different distributions? Without a real-world knowledge, this is hard to answer. 5 of the 7 sets of ones are 4-long, while only 2 of the 6 sets of zeros are 4-long. This might suggest different distributions (since $\\frac{5}{7}$ and $\\frac{2}{6}$ seem \"far off\"), but then again we might have simply not observed long enough. I think there is enough evidence to assume they are from different distributions (one might do MAP estimation to confirm).\n",
    "\n",
    "Now we have described the model in words, how can we formalize this? We can define a hidden variable. Let $n_t \\in \\{0, 1, 2, 3\\}$ represent the *number* of ones or zeros that will come after the current observation.\n",
    "\n",
    "Now, what depends on what? The current observation $y_t$ depends on $y_{t-1}$ and $n_{t-1}$: if we have $n_{t-1} \\gt 0$, then $y_t = y_{t-1}$, otherwise, $y_t = 1 - y_{t-1}$.\n",
    "\n",
    "The state $n_t$ depends on the previous state **and** the previous observation: if $n_{t-1} > 0$, $n_t = n_{t-1} - 1$. However, if $n_{t-1} = 0$, we choose the next number to be 3 or 4 depending on whether we'll observe ones or zeros, so we need that information. While it's true that most of the time, $n_t$ *does not* depend on $n_{t-1}$, it's impossible to enumerate which times that is, since that also depends on the previous $n_t$'s!\n",
    "\n",
    "Overall, the model is as follows:\n",
    "\n",
    "<img src=\"model5.png\">\n",
    "\n",
    "Assuming we start with ones, we have the following relations:\n",
    "\n",
    "Let $P_{zeros}$ be a probability distribution such that if $x \\sim P_{zeros}, p(x = 3) = \\frac{2}{3}$ and $p(x = 4) = \\frac{1}{3}$.\n",
    "\n",
    "Similarly, let $P_{ones}$ be a probability distribution such that if $x \\sim P_{ones}, p(x = 3) = \\frac{2}{7}$ and $p(x = 4) = \\frac{5}{7}$.\n",
    "\n",
    "(These are simple Bernoulli distributions, but I just wanted to give them names for easier reading.)\n",
    "\n",
    "$y_0 = 1$\n",
    "\n",
    "$n_0 = n \\sim P_{ones}$\n",
    "\n",
    "$y_t = \\begin{cases} \n",
    "      y_{t-1} , \\text{if } n_{t-1} \\gt 0 \\\\\n",
    "      1-y_{t-1} , \\text{if } n_{t-1} = 0\n",
    "   \\end{cases}\n",
    "$\n",
    "\n",
    "$n_t = \\begin{cases} \n",
    "      n_{t-1} - 1 , \\text{if } n_{t-1} \\gt 0 \\\\\n",
    "      n \\sim P_{ones} , \\text{if } n_{t-1} = 0 \\text{ and } y_{t-1} = 0 \\\\\n",
    "      n \\sim P_{zeros} , \\text{if } n_{t-1} = 0 \\text{ and } y_{t-1} = 1\n",
    "   \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18\n",
    "\n",
    "A distribution factorises according to the following factorisation\n",
    "$p(A, B, D, F, T, L, M, X) = p(F|T, L)p(M)p(T|A)p(B|M)p(X|F)p(L|M)p(D|F, B)p(A)$\n",
    "\n",
    "#### 1. Directed Graphical Model\n",
    "\n",
    "The directed graphical model is as follows:\n",
    "\n",
    "<img src=\"dag18.png\">\n",
    "\n",
    "#### 2. Undirected Graphical Model and Factor Graph\n",
    "\n",
    "Marrying the parents to show conditional dependence yields the undirected graphical model:\n",
    "\n",
    "<img src=\"ug18.png\">\n",
    "\n",
    "The factor graph is as follows:\n",
    "\n",
    "<img src=\"fg18.png\">\n",
    "\n",
    "where\n",
    "\n",
    "$f_1 = p(F | T, L)$\n",
    "\n",
    "$f_2 = p(M)$\n",
    "\n",
    "$f_3 = p(T | A)$\n",
    "\n",
    "$f_4 = p(B | M)$\n",
    "\n",
    "$f_5 = p(X | F)$\n",
    "\n",
    "$f_6 = p(L | M)$\n",
    "\n",
    "$f_7 = p(D | F, B)$\n",
    "\n",
    "$f_8 = p(A)$\n",
    "\n",
    "#### 3. Calculating Model Space\n",
    "\n",
    "If all variables have $N$ states, how much space do we need? We can just look at the factors.\n",
    "\n",
    "$A$ has $N$ values, so knowing the probabilities of any $N-1$ of them is enough. The same goes for $M$. This gives us $2(N-1) = 2N-2$.\n",
    "\n",
    "$T$ depends on $A$ - so we have $N$ columns with $N-1$ entries each (if we know $Pr(T = t_1 | A = a_1), ... , Pr(T = t_1 | A = a_{N-1})$, we can infer $Pr(T = t_1 | A = a_N).)$ - so $N(N-1)$ entries are required. The same goes for all single dependence cases, of which there are 5. So we have $5N(N-1) = 5N^2 - 5N$\n",
    "\n",
    "For double-dependences (such as $p(F | T, L)$), for each of the $N$ states that $F$ can take, we need $N^2-1$ entries for all possible $(T, L)$ pairs - we can omit the last one. Thus, we have $2N(N^2-1) = 2N^3 - 2N$.\n",
    "\n",
    "Adding all three, we get a total space requirement of $2N^3 + 5N^2 - 7N - 2$.\n",
    "\n",
    "#### 4. Analyzing conditional independence via d-separation\n",
    "\n",
    "**a)** $A \\perp\\!\\!\\!\\perp M$ $|$ $\\varnothing$ - **True**\n",
    "\n",
    "This is essentially the question \"Is A independent of M?\". We'd expect the answer to be \"yes\", since A and M seem like two independent causes. This is confirmed by d-separation. There are 2 paths from A to M, and both are blocked:\n",
    "\n",
    "Path A-T-F-L-M has collider F, and clearly F $\\notin \\varnothing$ (and any descendants of F are not in the empty set).\n",
    "\n",
    "Path A-T-F-D-B-M has colliders F and D, and the same argument is valid here too.\n",
    "\n",
    "Thus, $A \\perp\\!\\!\\!\\perp M$ $|$ $\\varnothing$ is **true**.\n",
    "\n",
    "**b)** $A \\perp\\!\\!\\!\\perp M$ $|$ $X$ - **False**\n",
    "\n",
    "This time, Path A-T-F-L-M has collider F, and X is a descendant of F. Since no conditioning variables are on the path itself, this path *d-connects* A and M.\n",
    "\n",
    "Therefore, the conditional independence is **false**, A and M are conditionally dependent on X.\n",
    "\n",
    "**c)** $T \\perp\\!\\!\\!\\perp L$ $|$ $X$ - **False**\n",
    "\n",
    "The path T-F-L has collider F, and X is a descendant of F. Since the only conditioning variable X is not on the path itself, this path *d-connects* T and L.\n",
    "\n",
    "Thus, conditional independence **does not hold**.\n",
    "\n",
    "**d)** $X \\perp\\!\\!\\!\\perp L$ $|$ $F$ - **True**\n",
    "\n",
    "There are two paths from X to L.\n",
    "\n",
    "Path X-F-L has no colliders. F blocks this path by being a non-collider in the conditioning set.\n",
    "\n",
    "Path X-F-D-B-M-L has collider D. This path is blocked by F for the same reason. (Even if it did not, D has no descendants in the conditioning set, so this would also block the path.)\n",
    "\n",
    "Since both paths are blocked, the conditional dependence **holds**.\n",
    "\n",
    "**e)** $X \\perp\\!\\!\\!\\perp L$ $|$ $D$ - **False**\n",
    "\n",
    "Path X-F-D-B-M-L is not blocked: the only element of the conditioning set, D, is a collider on this path. Thus, this path *d-connects* X and L, and the two are conditionally **dependent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Homework 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the .csv file via the given code, using the tried and tested method of shameless copy-pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "import numpy as np\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Generating Random Strings\n",
    "\n",
    "We first generate a cumulative table for easier random character generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cumT = []\n",
    "for row in T:\n",
    "    cumRow = []\n",
    "    currentSum = 0\n",
    "    for entry in row:\n",
    "        currentSum = currentSum + eval(entry)\n",
    "        cumRow.append(currentSum)\n",
    "    cumT.append(cumRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we write a function to generate a random single next character, given the previous character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate a random string of given length $n$. I assumed this was for all strings, so spaces are allowed. (This is realistic for large $n$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Example outputs for $\\{5, 10, 15, 20, 25\\}$ characters:"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.sa.\n",
      "ind.suling\n",
      "is.t.a.wabours.\n",
      "isorond.omain.n.tord\n",
      "thet.cad.hit.ingele.ugerz\n"
     ]
    }
   ],
   "source": [
    "## Generates a random string of a given length, using the Markov(1) model\n",
    "\n",
    "def generateRandomString(n):\n",
    "    currentChar = '.'\n",
    "    word = ''\n",
    "    for i in range(n):\n",
    "        currentChar = getNextChar(currentChar)\n",
    "        word = word + currentChar\n",
    "    return word\n",
    "\n",
    "display(Latex(\"Example outputs for $\\{5, 10, 15, 20, 25\\}$ characters:\"))\n",
    "for i in range(1,6):\n",
    "    print(generateRandomString(i*5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Filling Spaces (Naive)\n",
    "\n",
    "In this part, we fill omitted parts of text by sampling from the appropriate prior distribution. However, no posterior information is used (so only the last letter **before** the omitted part will be used, and the letter **after** the part will be gleefully disregarded). The input is assumed to be reduced to canonical form ('.' instead of punctuation and no capital letters). Spaces will be given by the underscore character '_' .\n",
    "\n",
    "The following function fills the empty spaces of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naiveFillSpaces(string):\n",
    "    chars = list(string)\n",
    "    prevChar = '.' # last known character\n",
    "    result = ''\n",
    "    for char in chars:\n",
    "        if (char != '_'):\n",
    "            result = result + char # add known character to result\n",
    "            prevChar = char # this is now the last known character\n",
    "        if (char == '_'): # this character is supposed to be filled\n",
    "            char = getNextChar(prevChar) # predict character\n",
    "            result = result + char # add predicted character to result\n",
    "            prevChar = char # this is now the last known character\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each result is actually iterated sampling of the distribution, so each time this function is called, it will give a different answer. The below code demonstrates this function on the 4 given test strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input string: th__br__n.f_x.\n",
      "Naive guess: th.bbrecn.fox.\n",
      "\n",
      "Input string: _u_st__n_.to_be._nsw_r__\n",
      "Naive guess: curst.tnc.tonbe.tnswhred\n",
      "\n",
      "Input string: i__at_._a_h_n_._e_r_i_g\n",
      "Naive guess: is.at..iaphanc.tecr.itg\n",
      "\n",
      "Input string: q___t.___z._____t.__.___.__.\n",
      "Naive guess: qurit.chez.in.t.t.se.y.i.pr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "\n",
    "for string in test_strings:\n",
    "    print(\"Input string: \" + string)\n",
    "    print(\"Naive guess: \" + naiveFillSpaces(string))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 3: Maximum Likelihood Estimator\n",
    "\n",
    "In this part, we ask the question: what is the most likely string to yield our partial string? We use a maximum likelihood estimator.\n",
    "\n",
    "First, let us go only left-to-right: not consider the characters **after** the missing parts, but only those **before**. This is much easier to implement and works in linear time because at each step we just do a table lookup and return the most likely letter.\n",
    "\n",
    "Below is a small function returning the most likely letter coming **after** a given letter, and the associated log-probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given a character whose next character is unknown, returns [nextChar, logProb]\n",
    "## where nextChar is the most likely next character and logProb its log-probability.\n",
    "\n",
    "def getNextCharMLE(prevChar):\n",
    "    row = T[letter2idx[prevChar]]\n",
    "    result = []\n",
    "    maxProb = 0\n",
    "    maxIndex = 0\n",
    "    for i in range(27):\n",
    "        if (eval(row[i]) > maxProb):\n",
    "            maxIndex = i\n",
    "            maxProb = eval(row[i])\n",
    "    j = \"\"\n",
    "    if maxIndex < 26:\n",
    "        result.append(chr(maxIndex+ord('a')))\n",
    "    else:\n",
    "        result.append('.')\n",
    "    result.append(np.log(maxProb))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the described \"greedy MLE\", and its results on the test strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for left-to-right greedy algorithm\n",
      "------------------------------------------\n",
      "Input string: th__br__n.f_x.\n",
      "Greedy MLE guess: the.bre.n.f.x. with log-probability -5.29868846243\n",
      "\n",
      "Input string: _u_st__n_.to_be._nsw_r__\n",
      "Greedy MLE guess: tursthen..tonbe.tnswhre. with log-probability -14.9727875596\n",
      "\n",
      "Input string: i__at_._a_h_n_._e_r_i_g\n",
      "Greedy MLE guess: in.ath.tanhen..te.reing with log-probability -15.1459084644\n",
      "\n",
      "Input string: q___t.___z._____t.__.___.__.\n",
      "Greedy MLE guess: quret.thez.the.tt.th.the.th. with log-probability -23.6505063019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def greedyMLE(string):\n",
    "    logProb = 0\n",
    "    chars = list(string)\n",
    "    prevChar = '.' # last known character\n",
    "    result = ''\n",
    "    for char in chars:\n",
    "        if (char != '_'):\n",
    "            result = result + char # add known character to result\n",
    "            prevChar = char # this is now the last known character\n",
    "        if (char == '_'): # this character is supposed to be filled\n",
    "            nextCharMLE = getNextCharMLE(prevChar)\n",
    "            char = nextCharMLE[0] # the first element is the character\n",
    "            logProb = logProb + nextCharMLE[1] # the second element is the log-probability\n",
    "            result = result + char # add predicted character to result\n",
    "            prevChar = char # this is now the last known character\n",
    "    return [result, logProb]\n",
    "\n",
    "print(\"Results for left-to-right greedy algorithm\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "for string in test_strings:\n",
    "    result = greedyMLE(string)\n",
    "    print(\"Input string: \" + string)\n",
    "    print(\"Greedy MLE guess: \" + str(result[0]) + \" with log-probability \" + str(result[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this does not work very well - the actual MLE will hopefully be much better! To be able to implement the MLE, we need to first get another table containing $p(x_{t-1} = j | x_t = i)$. This is simply T's columns written one by one, multiplied by prior probabilities, in rows, normalized. To estimate the prior probabilities, we use the estimate \"how likely is it to occur after other letters?\" - that is, $p(x_j) = \\frac{\\sum_{i=1}^{27} p(x_j | x_i)}{\\sum_{j=1}^{27} \\sum_{i=1}^{27} p(x_j | x_i)}$. We now compute this new table $P$, which combines both the structural information of the original table $T$ and the prior probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, compute the list of prior probabilities.\n",
    "# A letter is more possible to occur if it comes after other letters more likely.\n",
    "# Thus, we estimate this as the total\n",
    "\n",
    "priors = []\n",
    "for i in range (27):\n",
    "    total = 0\n",
    "    for j in range (27):\n",
    "        total = total + eval(T[j][i])\n",
    "    # we now have a legal total\n",
    "    priors.append(total)\n",
    "\n",
    "# priors is not normalized yet\n",
    "priorsTotal = sum(priors)\n",
    "for i in range (27):\n",
    "    priors[i] = priors[i] / priorsTotal\n",
    "\n",
    "# Compute the list of reverse-probabilities (right-to-left)\n",
    "P = []\n",
    "for i in range (27):\n",
    "    colT = [] # the relevant column of T\n",
    "    total = 0\n",
    "    for j in range (27):\n",
    "        colT.append(eval(T[j][i]) * priors[j]) # append column\n",
    "        total = total + eval(T[j][i]) * priors[j]\n",
    "    # we now have a correct total and the i'th row is ready to go\n",
    "    for j in range (27):\n",
    "        colT[j] = colT[j] / total # normalized probability\n",
    "    P.append(colT) # append this column as row to the other table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function to get the most likely character, given the character **after** it (this is simply the above function <code>getNextCharMLE</code> with the referred table as <code>P</code> instead of <code>T</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given a character whose previous character is unknown, returns [prevChar, logProb]\n",
    "## where prevChar is the most likely previous character and logProb its log-probability.\n",
    "\n",
    "def getPrevCharMLE(nextChar):\n",
    "    row = P[letter2idx[nextChar]]\n",
    "    result = []\n",
    "    maxProb = 0\n",
    "    maxIndex = 0\n",
    "    for i in range(27):\n",
    "        if (row[i] > maxProb):\n",
    "            maxIndex = i\n",
    "            maxProb = row[i]\n",
    "    j = \"\"\n",
    "    if maxIndex < 26:\n",
    "        result.append(chr(maxIndex+ord('a')))\n",
    "    else:\n",
    "        result.append('.')\n",
    "    result.append(np.log(maxProb))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our greedy algorithm does if it parses right-to-left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for right-to-left greedy algorithm\n",
      "------------------------------------------\n",
      "Input string: th__br__n.f_x.\n",
      "Greedy MLE guess: the.br.an.fex. with log-probability -4.32609506555\n",
      "\n",
      "Input string: _u_st__n_.to_be._nsw_r__\n",
      "Greedy MLE guess: ou.st.ane.to.be.answerhe with log-probability -11.6845731084\n",
      "\n",
      "Input string: i__at_._a_h_n_._e_r_i_g\n",
      "Greedy MLE guess: ie.ate..athane.heer.ing with log-probability -13.1542096521\n",
      "\n",
      "Input string: q___t.___z._____t.__.___.__.\n",
      "Greedy MLE guess: qhe.t.herz..the.t.he.the.he. with log-probability -22.3329059034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def greedyMLEBackwards(string):\n",
    "    logProb = 0\n",
    "    chars = list(string)\n",
    "    nextChar = '.' # last known character\n",
    "    result = ''\n",
    "    for i in range(len(chars) - 1, -1, -1): # loop backwards\n",
    "        char = chars[i]\n",
    "        if (char != '_'):\n",
    "            result = char + result # add known character to result\n",
    "            nextChar = char # this is now the last known character\n",
    "        if (char == '_'): # this character is supposed to be filled\n",
    "            prevCharMLE = getPrevCharMLE(nextChar)\n",
    "            char = prevCharMLE[0] # the first element is the character\n",
    "            logProb = logProb + prevCharMLE[1] # the second element is the log-probability\n",
    "            result = char + result # add predicted character to result\n",
    "            nextChar = char # this is now the last known character\n",
    "    return [result, logProb]\n",
    "\n",
    "print(\"Results for right-to-left greedy algorithm\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "for string in test_strings:\n",
    "    result = greedyMLEBackwards(string)\n",
    "    print(\"Input string: \" + string)\n",
    "    print(\"Greedy MLE guess: \" + str(result[0]) + \" with log-probability \" + str(result[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better actually, just looking at the log-probabilities! Of course, this still remains a greedy algorithm. \n",
    "\n",
    "We can merge the two approaches to \"meet in the middle\" - for example, for a space of 4 characters, there are 5 possible \"meeting points\" - either go all left-to-right (0-4), all right-to-left (4-0), or pick 1 letter from left to right and the rest from right to left (1-3), etc. This has complexity $n^2$ where $n$ is the number of missing characters - $n$ different possibilities, and $n$ table lookups for each case. (This can further be reduced since some lookups are shared among different cases.)\n",
    "\n",
    "The point is, though, we cannot afford a brute-force approach, since checking all possibilities entails $27^n$ complexity! If most spaces are short, this is affordable, and was probably the point of this assignment.\n",
    "\n",
    "We can again merge the two approaches above (go greedy until 3-4 spaces are left, then check all possibilities).\n",
    "\n",
    "My honest reaction to the above discussion at this point is that I was too late in starting this homework and am too tired to implement the mixed strategies described above. This is prone to change depending on your feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Improving the Model\n",
    "\n",
    "Of course, the simplicity of Markov(1) is irrefutable. While this gives lower computing complexity, its accuracy is questionable (and would be able to be questioned properly if the student was not too lazy to write an actual algorithm to measure this).\n",
    "\n",
    "An obvious \"improvement\" is higher-order Markov models. They are more accurate, while bringing a higher-order power to the equation.\n",
    "\n",
    "Another would be just checking a dictionary, learning words as a whole, but there's nothing Bayesian about the brute force solution. (If multiple \"sentences\" are legitimately probable, this is very unlikely. Then, a Bayesian algorithm could be designed to measure the semantics of the \"sentence\" (less occurrence in corpus -> less probability of being correct, etc.).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
