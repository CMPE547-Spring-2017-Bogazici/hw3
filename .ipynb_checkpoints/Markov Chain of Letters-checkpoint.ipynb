{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: [Sevgi Öztürk]\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to 16.3-5 and 18\n",
    "$16.3$\n",
    "<img src=\"16.3.png\">\n",
    "<img src=\"16.3.GM.png\">\n",
    "\n",
    "$16.4$\n",
    "<img src=\"16.4.png\">\n",
    "<img src=\"16.4.T.png\">\n",
    "<img src=\"16.4.GM.png\">\n",
    "\n",
    "$16.5$\n",
    "<img src=\"16.5.png\">\n",
    "<img src=\"16.5.T.png\">\n",
    "<img src=\"16.5.GM.png\">\n",
    "\n",
    "$18.1$\n",
    "<img src=\"18.1.jpg\">\n",
    "\n",
    "$18.2$\n",
    "<img src=\"18.2.jpg\">\n",
    "<img src=\"18.3.jpg\">\n",
    "\n",
    "$18.3$\n",
    "$$\n",
    "p(A, B, D, F, T, L, M, X) = p(F|T, L)p(M)p(T|A)p(B|M)p(X|F)p(L|M)p(D|F, B)p(A) \\\\ \n",
    "Parameters ; N^2 + (N-1) + N + N + N + N + N^2 + (N-1) = N^2 + 6N - 2\n",
    "$$\n",
    "\n",
    "$18.4$\n",
    "a) True. Since A and M are a priori unconditionally independent causes of the model.\n",
    "\n",
    "b) False. Since F is a collider and parent of X, which is in the path between A and M. Inclusion of a descendant of a collider violates d-seperation property, so A and M are not independent conditioned on X.\n",
    "\n",
    "c) False. Same violation in b is here. Because d-seperation is not possible here, they are not independent conditioned on X.\n",
    "\n",
    "d) True. Since in the path between X and L, there is no collider and F d-seperates X and L.\n",
    "\n",
    "e) True. Inclusion of a descendant of a non-collider does not violate d-seperation property, so X and L are independent conditioned on D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Solution\n",
    "\n",
    "$1$. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "\n",
    "Since the only condition is to start with $x_0$, bunch of chars are generated directly from transition table $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprcfqibgk\n",
      "bclqqincni\n",
      "rbxxmnaohb\n",
      "zoavjqnnoz\n",
      "krkfajfwsg\n",
      "ijgyeksikm\n",
      "tzwekevhut\n",
      "qxeqzgnebn\n",
      "chcvhtltmp\n",
      "udkawrpoeo\n",
      "maoqsqkoib\n",
      "qwgesbqpnv\n",
      "ttabbbqpxo\n",
      "qakslrzvug\n",
      "vknpxlcesn\n",
      "pxipemacvp\n",
      "yegxiqiokl\n",
      "ltscpbgqgm\n",
      "gqpnizsysp\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import csv\n",
    "import numpy as np\n",
    "import functools as fnt\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphRange = 27\n",
    "alphabet = [chr(i+ord('a')) for i in range(alphRange-1)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "\n",
    "ind = 26 # take . index as prior knowledge for x0\n",
    "lenStr = 10 # generate string of that length\n",
    "times = 20  # repeat the operation that times\n",
    "for i in range(1,times):\n",
    "    T1 = T[ind] # start generating strings as if they start with .\n",
    "    p = [float(x) for x in T1]\n",
    "    rndStr = ''\n",
    "    for j in range(lenStr):\n",
    "        rndInt = np.random.choice(ind,1, p) # generate string as char array with length=lenStr acc.to given probabilities\n",
    "        rndChar = alphabet[rndInt[0]]\n",
    "        rndStr +=rndChar\n",
    "        T1 = T[rndInt[0]] # update acc.to new upcoming char\n",
    "        p = [float(x) for x in T1] # update probability distribution\n",
    "    print(rndStr)\n",
    "    #print(''.join(str), ' with p(x1..N|x0) :',fnt.reduce(lambda x, y: x*y, jointPr)) # multiply all prob. by implementing ad-hoc function here\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below.\n",
    "\n",
    "Since Markov(1) will be used as stated above, known characters will be regarded as prior knowledge to guess the new distribution of unknown characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution dimension :  9  normFactor: 2.54398597523e-05  sum: 7.77796352128e-07\n",
      "Str :  th__br__n.f_x.  Proposed Str :  the.brtan.fux.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sevgi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:71: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution dimension :  11  normFactor: 0.000578384585705  sum: 1.54860168139e-05\n",
      "Str :  _u_st__n_.to_be  Proposed Str :  junstwand.to.be\n",
      "Distribution dimension :  12  normFactor: 0.131796  sum: 4.65641243077e-06\n",
      "Str :  i__at_._a_h_n  Proposed Str :  itgatr.wachen\n",
      "Distribution dimension :  9  normFactor: 0.244947  sum: 5.82353655356e-05\n",
      "Str :  q___t.___z  Proposed Str :  qutst.thez\n"
     ]
    }
   ],
   "source": [
    "def axis_without(original_tuple, elementInd):\n",
    "    new_tuple = []\n",
    "    ind = 0\n",
    "    for ind in range(len(original_tuple)): #list(original_tuple):\n",
    "        if not ind == elementInd:\n",
    "            new_tuple.append(ind)\n",
    "    return tuple(new_tuple)\n",
    "\n",
    "# Original strings here: Local memory was not enough after the first one, so I reduced the strings a bit !\n",
    "#test_strings = ['th__br__n.f_x.','_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "test_strings = ['th__br__n.f_x.','_u_st__n_.to_be','i__at_._a_h_n','q___t.___z']\n",
    "\n",
    "# Sample Outputs\n",
    "#Str :  th__br__n.f_x.  Proposed Str :  the.brsin.fax.\n",
    "#Str :  _u_st__n_.to_be  Proposed Str :  qulsthine.to.be\n",
    "#Str :  i__at_._a_h_n  Proposed Str :  itoate..athan\n",
    "#Str :  q___t.___z  Proposed Str :  qumat.eloz\n",
    "        \n",
    "arrT = np.array([[float(y) for y in x] for x in T])\n",
    "dimStr = 'ijklmnoprstuvyz'\n",
    "\n",
    "for i in range(len(test_strings)):\n",
    "    str = list(test_strings[i])\n",
    "    prevDistr = np.array([1])\n",
    "    prevDistr = prevDistr[0]\n",
    "    normFactor = 1\n",
    "\n",
    "    for j in range(len(str)-1): # focus on edges NOT vertices\n",
    "        \n",
    "        # Type 1: both vertices are known\n",
    "        if str[j]!='_' and str[j+1]!='_':\n",
    "            distr = arrT[letter2idx[str[j]],letter2idx[str[j+1]]]  # point(th) = 0-D \n",
    "        # Type 2: first vertex is known second one is unknown\n",
    "        elif str[j]!='_' and str[j+1]=='_':  # line(h_) = 1-D row\n",
    "            distr = arrT[[letter2idx[str[j]]],:]\n",
    "        # Type 3: first vertex is unknown second one is known\n",
    "        elif str[j]=='_' and str[j+1]!='_':  # line(_b) = 1-D column\n",
    "            distr = arrT[:,[letter2idx[str[j+1]]]]\n",
    "        # Type 4: both vertices are unknown --> dimension increase\n",
    "        elif str[j]=='_' and str[j+1]=='_':  # surface(__) = 2-D\n",
    "            distr = arrT\n",
    "        \n",
    "        if distr.ndim == 0:\n",
    "            normFactor = normFactor*distr\n",
    "            \n",
    "        #print('chars :',str[j],str[j+1])\n",
    "        if prevDistr.ndim == 0 or distr.ndim == 0: # scalar 0-D\n",
    "            distr = prevDistr*distr                 \n",
    "        else:\n",
    "            preStr = dimStr[0:prevDistr.ndim]\n",
    "            postStr = dimStr[prevDistr.ndim-1:prevDistr.ndim+distr.ndim-1]                        \n",
    "            adhocStr = preStr+','+postStr+'->'+ preStr+postStr[1 :] # like 'ij,jk->ijk'\n",
    "            #print('preStr:',preStr,' postStr:',postStr, ' adhocStr:', adhocStr)\n",
    "            distr = np.einsum(adhocStr,prevDistr,distr)\n",
    "            \n",
    "        #print('j :',j,' prevDistr.shape : ',prevDistr.shape, ' distr.shape : ',distr.shape)        \n",
    "        prevDistr = distr    \n",
    "        \n",
    "    distr = distr/normFactor\n",
    "    s = np.sum(distr)    \n",
    "    print('Distribution dimension : ', distr.ndim)\n",
    "    \n",
    "    propStr = str\n",
    "    for k in range(distr.ndim):\n",
    "        sh = distr.shape\n",
    "        if sh[k]==alphRange: # unknown char dimension\n",
    "             cumDims = axis_without(sh,k)\n",
    "             cumDistr = np.sum(distr,axis=cumDims)\n",
    "             sample = np.random.multinomial(1,cumDistr/sum(cumDistr))\n",
    "             indx = np.nonzero(sample)\n",
    "             newChar = alphabet[indx[0]]\n",
    "             ii = propStr.index('_')\n",
    "             propStr[ii] = newChar\n",
    "                    \n",
    "    print('Str : ',test_strings[i], ' Proposed Str : ', ''.join(propStr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$. It needs to get the maximum log probability on relevant dimension in the above code, but I had no time left !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$.Discuss how you can improve the model to get better estimations.\n",
    "\n",
    "Instead of using Markov(1), it can be developed by using Markov(N), which contributes more to the following vertices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
