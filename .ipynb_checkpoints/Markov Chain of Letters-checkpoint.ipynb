{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Berk Görgülü\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_t=\\left\\{\\begin{array}{lr} S_{t-1} &w.p. &p\\\\ Uniform(0,30) &w.p &1-p\\end{array}\\right\\} \\ where\\ p\\gg 1-p$$\n",
    "\n",
    "$$Y_t=\\left\\{\\begin{array}{lr} S_{t} &w.p. &q\\\\ Uniform(0,30) &w.p &1-q\\end{array}\\right\\} \\ where\\ q\\gg 1-q $$\n",
    "<br\\>\n",
    "\n",
    "$$\\begin{array}{rr} \\hline\n",
    "P\\left(S_t\\ \\middle|\\ S_{t-1}\\right)  &S_{t-1}\\\\ \\hline\n",
    "S_t=S_{t-1} &p \\\\ \\hline\n",
    "S_t=Uniform(0,30) &1-p \\\\ \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{rr} \\hline\n",
    "P\\left(Y_t\\ \\middle|\\ S_t\\right)  &S_{t-1}\\\\ \\hline\n",
    "Y_t=S_t &q \\\\ \\hline\n",
    "Y_t=Uniform(0,30) &1-q \\\\ \\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Selection_006.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{rr} \\hline\n",
    "P\\left(S_t\\ \\middle|\\ S_{t-1}\\right)  &S_{t-1}=a &S_{t-1}=b &S_{t-1}=c &S_{t-1}=d &S_{t-1}=e \\\\ \\hline\n",
    "S_t=a &1-p &1-q &1-q &1-q &m \\\\ \\hline\n",
    "S_t=b &p &0 &0 &0 &0 \\\\ \\hline\n",
    "S_t=c &0 &q &0 &0 &0 \\\\ \\hline\n",
    "S_t=d &0 &0 &q &0 &0 \\\\ \\hline\n",
    "S_t=e &0 &0 &0 &q &1-m \\\\ \\hline\n",
    "\\end{array}$$\n",
    "$$\\ where\\ p\\gg 1-p, \\\\ q\\gg 1-q,\\\\  m\\gg 1-m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Selection_009.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Approach (Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Since\\ same\\ numbers\\ appear\\ in\\ a\\ batch\\ of\\ 3\\ or\\ 4\\ neither\\ more\\ nor\\ less. \\\\ A\\ model\\ is\\ needed\\ such\\ that\\ a\\ state\\ should\\ be\\ aware\\ of\\ previous\\ 4\\ state\\ in\\ order\\ to\\ perform\\ this\\ rule. \\\\ \\ So\\ I\\ suggest\\ Markov(4)\\ model\\ for\\ this\\ problem.$\n",
    "\n",
    "$$\\begin{array}{rr} \\hline\n",
    "P\\left(S_t\\ \\middle|\\ S_{t-1},S_{t-2},S_{t-3},S_{t-4}\\right)  &S_{t-1}=S_{t-2}=S_{t-3}=S_{t-4}=i &S_{t-1}=S_{t-2}=S_{t-3}=i, S{t-4}=j &Other\\ Cases\\ when\\ S_{t-1}=i \\\\ \\hline\n",
    "S_t=i &0 &p &1\\\\ \\hline\n",
    "S_t=j &1 &1-p &0 \\\\ \\hline\n",
    "\\end{array}$$\n",
    "$$\\ where\\ p\\gg 1-p $$\n",
    "$$i,j \\in \\left\\{0,1 \\right\\}$$\n",
    "$$i+j=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Selection_19.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Approach (Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{rr} \\hline\n",
    "P\\left(S_t\\ \\middle|\\ S_{t-1}\\right)  &S_{t-1}= 1 &S_{t-1}>1 \\\\ \\hline\n",
    "S_t= U\\left\\{3,4\\right\\} &1 &0\\\\ \\hline\n",
    "S_t=S_{t-1}-1 &0 &1\\\\ \\hline\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\begin{array}{rr} \\hline\n",
    "P\\left(Y_t\\ \\middle|\\ Y_{t-1},S_t\\right)  &S_{t}= 1, Y_{t-1}=i &S_{t}>1 \\\\ \\hline\n",
    "Y_t= j &1 &0\\\\ \\hline\n",
    "Y_t= Y_{t-1} &0 &1\\\\ \\hline\n",
    "\\end{array}$$\n",
    "\n",
    "$$i,j \\in \\left\\{0,1 \\right\\}$$\n",
    "$$i+j=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Selection_006.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed Graphical Model\n",
    "<img src='Selection_011.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undirected Graphical Model\n",
    "<img src='Selection_018.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Graphical Model\n",
    "<img src='18-2Factor.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P\\left(A,B,D,F,T,L,M,X\\right)=P\\left(F\\ \\middle|\\ T,L\\right)P\\left(D\\ \\middle|\\ F,B\\right)P\\left(T\\ \\middle|\\ A\\right)P\\left(B\\ \\middle|\\ M\\right)P\\left(X\\ \\middle|\\ F\\right)P\\left(L\\ \\middle|\\ M\\right)P\\left(A\\right)P\\left(M\\right)$$\n",
    "\n",
    "$$P\\left(F\\ \\middle|\\ T,L\\right) -> (N-1)*N^2$$\n",
    "$$P\\left(D\\ \\middle|\\ F,B\\right) -> (N-1)*N^2$$\n",
    "$$P\\left(T\\ \\middle|\\ A\\right) -> (N-1)*N$$\n",
    "$$P\\left(B\\ \\middle|\\ M\\right) -> (N-1)*N$$\n",
    "$$P\\left(X\\ \\middle|\\ F\\right) -> (N-1)*N$$\n",
    "$$P\\left(L\\ \\middle|\\ M\\right) -> (N-1)*N$$\n",
    "$$P\\left(A\\right) -> N-1$$\n",
    "$$P\\left(M\\right) -> N-1$$\n",
    "\n",
    "$$TOTAL= 2N^3+2N^2-2N-2 = 2(N+1)^2(N-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A{\\perp\\!\\!\\!\\perp}M\\mid\\emptyset\\ ?$$\n",
    "<img src='Selection_013.png'>\n",
    "$$Possible Paths$$\n",
    "\n",
    "$$A->T->F<-L<-M$$\n",
    "At T arrows meet head to tail, since T is not given, it does not block the path. Arrows on this path also meet head to head at node F. Since there is not any conditioning made on the node F or descendant of the node F, the path is blocked by F. A and M are separated for this path.<br>\n",
    "\n",
    "$$A->T->F->D<-B<-M$$\n",
    "At T arrows meet head to tail, since T is not given, it does not block the path. After that arrows meet at F again head to tail. Since F is not given, the path still is not blocked. After F, arrows on this path meet head to head at node D. Since there is not any conditioning made on the node D or descendant of the node D, the path is blocked by D. A and M are separated again for this path.<br>\n",
    "\n",
    "Conclusion: Since they are separated for both paths, A and M are d-separated. They are independent.<br>\n",
    "$$A{\\perp\\!\\!\\!\\perp}M\\mid\\emptyset\\ -> TRUE$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A{\\perp\\!\\!\\!\\perp}M\\mid X\\ ?$$\n",
    "<img src='Selection_014.png'>\n",
    "$$Possible Paths$$\n",
    "$$A->T->F<-L<-M$$\n",
    "At T arrows meet head to tail, since T is not given, it does not block the path. Arrows on this path also meet head to head at node F. Conditioning is made on the node X which is a descendant of the node F. Therefore, the path between A and M is not blocked. This means that they are not separated.<br>\n",
    "\n",
    "$$A->T->F->D<-B<-M$$\n",
    "At T arrows meet head to tail, since T is not given, it does not block the path. After that arrows meet at F again head to tail. Since F is not given, the path still is not blocked. After F, arrows on this path meet head to head at node D. Since there is not any conditioning made on the node D or descendant of the node D, the path is blocked by D. A and M are separated for this path.<br>\n",
    "\n",
    "Conclusion: Since there exists a path between A and M which is not blocked, A and M are not d-separated given X. Therefore, they are not conditionally independent.\n",
    "$$A{\\perp\\!\\!\\!\\perp}M\\mid X\\ -> FALSE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$T{\\perp\\!\\!\\!\\perp}L\\mid X\\ ?$$\n",
    "<img src='Selection_015.png'>\n",
    "$$Possible Paths$$\n",
    "$$T->F<-L$$\n",
    "Arrows on this path meet head to head at node F. Conditioning is made on the node X which is a descendant of the node F. Therefore,this path between T and L is not blocked. So, they are not separated for this path<br>\n",
    "\n",
    "$$T->F->D<-B<-M->L$$\n",
    "Arrows meet head to tail at node F. Since F is not given, it does not block the path. After that arrows meet head to head at node D. Since D itself and any descendant of node D is not given, the path is blocked by node D. T and L are separated for this path.\n",
    "\n",
    "Conclusion: Since there exists a path between T and L which is not blocked, T and L are not d-separated given X. Therefore, they are not conditionally independent.\n",
    "$$T{\\perp\\!\\!\\!\\perp}L\\mid X\\ -> FALSE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X{\\perp\\!\\!\\!\\perp}L\\mid F\\ ?$$\n",
    "<img src='Selection_016.png'>\n",
    "$$Possible Paths$$\n",
    "$$X<-F<-L$$\n",
    "Arrows on this path meet head to tail at node F. Conditioning is made on the node F therefore it blocks the path. Because of that, L and X are separated for this path.<br>\n",
    "\n",
    "$$X<-F->D<-B<-M->L$$\n",
    "Arrows on this path meet tail to tail at node F, since node F is not given, path is not blocked by node F. Arrows on this path also meet head to head at node D. Conditioning is not made on the node D or its descendant nodes therefore the path is blocked. Therefore, L and X are separated for this path.<br>\n",
    "\n",
    "Conclusion: Since X and L are separated for both paths. They are d-separated given X. So X and L are conditionally independent.\n",
    "$$X{\\perp\\!\\!\\!\\perp}L\\mid F\\ -> TRUE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X{\\perp\\!\\!\\!\\perp}L\\mid D\\ ?$$\n",
    "<img src='Selection_017.png'>\n",
    "$$Possible Paths$$\n",
    "$$X<-F<-L$$\n",
    "Arrows on this path meet head to tail at node F. Conditioning is not made on the node F so it does not block the path. Therefore, L and X are not separated for this path.<br>\n",
    "\n",
    "$$X<-F->D<-B<-M->L$$\n",
    "Arrows on this path meet tail to tail at node F. Since conditioning is not made on the node F, the path is not blocked by F. The arrows also meet head to head at node D. Since D is given, the path again is not blocked by D. After that the arrows meet head to tail at node B. Since node B again is not given, the path is not blocked by node B. At last the arrwos meet tail to tail at node M. Since node M is not given, the path is not blocked by node M. Therefore  X and L are not separated for this path.\n",
    "\n",
    "Conclusion: Since there exists an unblocked path between X and L, they are not d-separated given D. So they are not conditionally independent.\n",
    "$$X{\\perp\\!\\!\\!\\perp}L\\mid D\\ -> FALSE$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '0.0002835')\n",
      "('b', '0.0228302')\n",
      "('c', '0.0369041')\n",
      "('d', '0.0426290')\n",
      "('e', '0.0012216')\n",
      "('f', '0.0075739')\n",
      "('g', '0.0171385')\n",
      "('h', '0.0014659')\n",
      "('i', '0.0372661')\n",
      "('j', '0.0002353')\n",
      "('k', '0.0110124')\n",
      "('l', '0.0778259')\n",
      "('m', '0.0260757')\n",
      "('n', '0.2145354')\n",
      "('o', '0.0005459')\n",
      "('p', '0.0195213')\n",
      "('q', '0.0001749')\n",
      "('r', '0.1104770')\n",
      "('s', '0.0934290')\n",
      "('t', '0.1317960')\n",
      "('u', '0.0098029')\n",
      "('v', '0.0306574')\n",
      "('w', '0.0088799')\n",
      "('x', '0.0009562')\n",
      "('y', '0.0233701')\n",
      "('z', '0.0018701')\n",
      "('.', '0.0715219')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Sampling Random Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Sample String Given $S_0$ only"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itaheceilomte.aslpeey.ht..cae.cm.otht.au.uhwdn.hhdategnarertdltlaoso.r.eeyle.iut.suig.m.ssd.nrle.rario.i.sa.t.ee.tbuu.tdaaaf.oo.dhr.payeyl.upeadadetpab.pefuer..fns.eabnrqh.rkousgbiseubgyeso.gbtv..aer..p.slnro..l.edeoa..aeid.i.r...noyhnrarmyh.rewramt.a.i.r.rshru.ut.htnkllhsm.temiy..sgf.oyrtovqnibic.e..trenierh..fdeotef.lredonesoseueowai..o....cstryavtgfsiynw.he.o.s.ooeiaeuai.ttenr.efgunaoeyhar.kuoo.yatn.egcmdsytvi.m.secabndaulunnnnyl.avdraoa..y.eees..tt.lfed.eg.h.goh.imo.at.aoi.ptnsharho.mfi....eceherl.etfriaesooer..r..racbni.e.v.ssnaintnneauh.eioundhotvdao.attn.san..strhpti.euhvmzievhtueeeeiadtycee.toh.m.tnd.eemletesuerusrht.mwa.elhc.iondebogidwd.eraweoteoeute.a.plut.ha..totwivurltaw.fdeh.ohehrinn.mitenhs.nwnsdm.rvoo.br.oe.fto.s.a.ch.a..nfyne..n.mtloe.hdeatpyoxtwlsehelfl.h.si.sfymiitg.hon..er.n.hrteal.iweietttftemiiastrseeo.enmoeig.ymreoh.tsdn..sespnmr.ur.a.aemdsahatsieuiynoasiemkn.iwo.if.a.pi.aoares..ntwnoeietnn.olgblaho.dtoweclyevhealksbeadslhp..t.abni.no.n.fgioetec.elrob...hoip.hsiw\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Sample String Given $S_{i-1}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.t.meancoun.odeenclloowhave..alan.and.tere..qum.le.se.s.the.aitoren.icthr.morr.g.ofiomaito.hetof..ok.an.ps.t.hatharalycea.on.nhemequrte.t.teriris.s.couthere.pridequtoupaligatuly.vethe.huthite.d.sof.t.r.pere.i.ndepass.wanin.mem.athin.be.onaldit.st.mem.ine.imred.tomothave.fthedidry.sasthee..wf.ovet.argr.in.ss.iereivety.s.ugha.e.alld.onoris.sumo.p.lubstammuck.s.am.tow...beca.atsthel.taneaven.he..o.whe.lm.thallit.ct..acancen.merse.t.whedi.g.w.ctolffofff.ozasaps.aleablamonethinthed.t.ct.gheere.m.st.bothanithelo.crecct.g.thexcamasthofimed.asear.umat.ve.whes.st.n.ge.the.ad.e.oted.idwig...thimeve.f.yant.cth.ad.pif.aneve.wing.cthe.ees.se.m.off.s.he.pere..asothine.y.d.tyowextwho.pahake.al..p.a.hovin.f.r.atoreneitid.oupi.a.whewathe.whth.lea.ms.te.wi.grion.acain.borfficothie.trinivino..s.h.tn.t.bl.th.fo.t.andoustheston.f..ates.f.re.ard.f.tir.toon.she.butice.ansh.ciomin.rloorshos.er.pris.d..ald.a.ithe.pllligedes.id.pit.verta.ct.ther.l.he.scheske.s.butremestht.t.o.pontede.chal.nd.zone.fid.llt.le.a.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#Sum of the probabilites for some columns are sometimes slightyly more than 1.(ex. 1.0000523)\n",
    "#Normalize function normalizes the probabilities to exactly 1\n",
    "def Normalize(K):\n",
    "    for i in range(len(K)):\n",
    "        temp=map(float,K[i])\n",
    "        K[i] = [float(j)/sum(temp) for j in temp]\n",
    "    return K\n",
    "\n",
    "#Samples from the distribution applying Markov(1) and returns a sequence with given number of letters\n",
    "#sampleString generates letters one by one and takes the last one as given in order to generate the next one\n",
    "np.random.seed(1)\n",
    "\n",
    "def sampleString(N):\n",
    "    s='.'\n",
    "    seq=''\n",
    "    K=Normalize(T)\n",
    "    for n in range(0,N):\n",
    "        probs=K[letter2idx[s]]\n",
    "        s = alphabet[np.random.choice(np.arange(0, 27), p=probs)]\n",
    "        seq=seq+s\n",
    "    return seq\n",
    "\n",
    "#Samples from the distribution applying Markov(1) and returns a sequence with given number of letters\n",
    "#sampleString generates letters takes only the '.' as given and generate each sequence by multiplying the transition probabilities\n",
    "def sampleStringFromInitial(N):\n",
    "    s='.'\n",
    "    seq=''\n",
    "    K=Normalize(T)\n",
    "    TransitionM = np.vstack(K).astype(float)\n",
    "    for n in range(0,N):\n",
    "        seq = seq + alphabet[np.random.choice(np.arange(0, 27), p=TransitionM[letter2idx[s]])]\n",
    "        TransitionM = np.dot(TransitionM,K)\n",
    "    return seq\n",
    "\n",
    "display(Latex(r'Sample String Given $S_0$ only')) \n",
    "print sampleStringFromInitial(1000)\n",
    "\n",
    "\n",
    "display(Latex(r'Sample String Given $S_{i-1}$')) \n",
    "print sampleString(1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Fill the Missing Letters by Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Define\\ all\\ letter\\ sequences\\ as\\ state\\ S_t\\\\\n",
    "Since\\ we\\ use\\ Markov(1)\\ model\\ we\\ only\\ need\\ to\\ look\\ at\\ the\\ closest\\ given\\ letters\\ from\\ both\\ sides\\ to\\ fill\\ the\\ missing\\ letters.\\\\\n",
    "Assume\\ that\\ S_i\\ is\\ not\\ given\\ and\\ we\\ want\\ to\\ predict\\ S_i\\\\ S_l\\ is\\ closest\\ given\\ letter\\ to\\ S_i\\ from\\ left\\ side\\ and\\ S_r\\ is\\ the\\ closest\\ given\\ letter\\ from\\ the\\ right\\ side.$\n",
    "\n",
    "$$Then:$$\n",
    "\n",
    "$$If\\ both\\ S_l\\ and\\ S_r\\ exist:$$\n",
    "\n",
    "$$P\\left(S_i\\ \\middle|\\ S_l=l,S_r=r\\right) = \\frac{P\\left(S_i,S_l=l,S_r=r\\right)}{P\\left(S_l=l,S_r=r\\right)}$$\n",
    "\n",
    "$$= \\frac{P\\left(S_l=l\\right)*P\\left(S_i\\ \\middle|\\ S_l=l\\right)*P\\left(S_r=r\\ \\middle|\\ S_i\\right)}{P\\left(S_r=r\\ \\middle|\\ S_l=l\\right)*P\\left(S_l=l\\right)}$$\n",
    "\n",
    "$$= \\frac{P\\left(S_i\\ \\middle|\\ S_l=l\\right)*P\\left(S_r=r\\ \\middle|\\ S_i\\right)}{P\\left(S_r=r\\ \\middle|\\ S_l=l\\right)}$$\n",
    "\n",
    "$$If\\ only\\ S_l\\ exists:$$\n",
    "\n",
    "$$P\\left(S_i\\ \\middle|\\ S_l=l\\right) = \\frac{P\\left(S_i,S_l=l\\right)}{P\\left(S_l=l\\right)}$$\n",
    "\n",
    "$$= \\frac{P\\left(S_l=l\\right)*P\\left(S_i\\ \\middle|\\ S_l=l\\right)}{P\\left(S_l=l\\right)}$$\n",
    "\n",
    "$$=P\\left(S_i\\ \\middle|\\ S_l=l\\right) \\\\$$\n",
    "\n",
    "$Since\\ Markov(1)\\ model\\ is\\ used\\ ,conditional\\ probabilities\\ represented\\ above\\ are\\ calculated\\ by\\ taking\\ the\\ powers\\ of\\ the\\ transition\\\\ matrix\\ depending\\ on\\ the\\ distance\\ between\\ the\\ indexes\\ of\\ given\\ letters\\ and\\ the\\ indexes\\ of\\ the\\ empty\\ letters.$\n",
    "\n",
    "$In\\ this\\ part,\\ after\\ calculating\\ the\\ desired\\ probability,\\ sampling\\ is\\ done\\ from\\ the\\ obtained\\ probability\\ distribution.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.Code & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  1   Sample  1  :  the.briin.fix.\n",
      "Text  1   Sample  2  :  the.brlin.fux.\n",
      "Text  1   Sample  3  :  therbrdon.fix.\n",
      "Text  1   Sample  4  :  the.brean.fex.\n",
      "Text  1   Sample  5  :  thi.brn.n.fex.\n",
      "Text  1   Sample  6  :  the.brein.fex.\n",
      "Text  1   Sample  7  :  the.brren.fex.\n",
      "Text  1   Sample  8  :  theabr.an.fex.\n",
      "Text  1   Sample  9  :  thi.brdin.fex.\n",
      "Text  1   Sample  10  :  the.br.an.fex.\n",
      "Text  1   Sample  11  :  theobr.un.fex.\n",
      "Text  1   Sample  12  :  thiubrean.fex.\n",
      "Text  1   Sample  13  :  thoibrean.fex.\n",
      "Text  1   Sample  14  :  the.brean.fex.\n",
      "Text  1   Sample  15  :  the.bregn.fex.\n",
      "Text  1   Sample  16  :  the.brion.fex.\n",
      "Text  1   Sample  17  :  the.brlon.fex.\n",
      "Text  1   Sample  18  :  tho.brpon.fex.\n",
      "Text  1   Sample  19  :  the.brean.fex.\n",
      "Text  1   Sample  20  :  the.brien.fix.\n",
      "Text  2   Sample  1  :  murstrong.torbe.unswirei\n",
      "Text  2   Sample  2  :  suestiant.toube.knswarm.\n",
      "Text  2   Sample  3  :  mugstlind.toube.onsworts\n",
      "Text  2   Sample  4  :  ru.staeng.to.be.answeren\n",
      "Text  2   Sample  5  :  oursthing.toube.answaree\n",
      "Text  2   Sample  6  :  ourstheng.tonbe.answirit\n",
      "Text  2   Sample  7  :  bubsthind.toobe.insweryd\n",
      "Text  2   Sample  8  :  ounstoane.tombe.answeres\n",
      "Text  2   Sample  9  :  oussth.ne.toube.onswergo\n",
      "Text  2   Sample  10  :  bunsteend.to.be.answerer\n",
      "Text  2   Sample  11  :  punstoan..torbe.wnswarn.\n",
      "Text  2   Sample  12  :  oussthiny.toube.inswario\n",
      "Text  2   Sample  13  :  suestring.toibe.answarat\n",
      "Text  2   Sample  14  :  sursthind.toube.insworel\n",
      "Text  2   Sample  15  :  qumsteand.to.be.inswor.i\n",
      "Text  2   Sample  16  :  qulsthend.toobe.answar.p\n",
      "Text  2   Sample  17  :  tussthing.toube.inswirea\n",
      "Text  2   Sample  18  :  fusst.ind.to.be.inswir..\n",
      "Text  2   Sample  19  :  outstsint.to.be.onswerc.\n",
      "Text  2   Sample  20  :  ourst.ant.tombe.inswersm\n",
      "Text  3   Sample  1  :  imraty.pathen..seer.ing\n",
      "Text  3   Sample  2  :  ithate.gathind.perrring\n",
      "Text  3   Sample  3  :  ishath.lathang.be.rwing\n",
      "Text  3   Sample  4  :  ie.at..hathen..rerr.ing\n",
      "Text  3   Sample  5  :  it.ats.pathind.nedr.ing\n",
      "Text  3   Sample  6  :  imdath.pathond.he.rving\n",
      "Text  3   Sample  7  :  imrato.eachane.berrding\n",
      "Text  3   Sample  8  :  ineath.ba.hind.beersing\n",
      "Text  3   Sample  9  :  im.atr.gathind.nearaing\n",
      "Text  3   Sample  10  :  in.ath.cathend.we.rking\n",
      "Text  3   Sample  11  :  ir.ath.rashans.cedr.ieg\n",
      "Text  3   Sample  12  :  ichath.jathand.pear.ing\n",
      "Text  3   Sample  13  :  iegats.lathond.wearding\n",
      "Text  3   Sample  14  :  itoato.fasheno.ie.r.ing\n",
      "Text  3   Sample  15  :  iliats..athand.neorping\n",
      "Text  3   Sample  16  :  iseaty.watheni.he.r.ing\n",
      "Text  3   Sample  17  :  inhath.pachind.heerming\n",
      "Text  3   Sample  18  :  ifeati.raphand.tearting\n",
      "Text  3   Sample  19  :  inhat..patheng.mearming\n",
      "Text  3   Sample  20  :  ieiate.wathand.reareing\n",
      "Text  4   Sample  1  :  qur.t.cszz.cfoh.t.ae.t.y.we.\n",
      "Text  4   Sample  2  :  qudut.wnrz.c.eaot.n..ahd.ie.\n",
      "Text  4   Sample  3  :  qum.t.t.az.oe.aat.hh.fcd.h..\n",
      "Text  4   Sample  4  :  qusit.aaez.muthst.tt.had.se.\n",
      "Text  4   Sample  5  :  qug.t.nniz.nahhit.an.t.n.oe.\n",
      "Text  4   Sample  6  :  que.t.teoz.pewryt.r..o.h.oy.\n",
      "Text  4   Sample  7  :  qul.t.ihrz.cn.ift.it.tya.e..\n",
      "Text  4   Sample  8  :  qup.t.brrz.d.es.t.no.bho.ir.\n",
      "Text  4   Sample  9  :  qug.t.i.rz.tnohct.he.ahe.ud.\n",
      "Text  4   Sample  10  :  qus.t.fa.z.shnrnt.us..iy.kr.\n",
      "Text  4   Sample  11  :  qusot.mvrz.y.ee.t.so.wff.le.\n",
      "Text  4   Sample  12  :  qusit.n.iz.ayna.t.iy.iae.hs.\n",
      "Text  4   Sample  13  :  qun.t.odaz.s.wt.t.td.frt.pe.\n",
      "Text  4   Sample  14  :  qutat.derz.uoelct.ws.nld.he.\n",
      "Text  4   Sample  15  :  qurit.eeiz.ihsant.sy.tee.bw.\n",
      "Text  4   Sample  16  :  qusit.atrz.ah.eat.oy.bns.ba.\n",
      "Text  4   Sample  17  :  qubit.sfaz.ohe.et.sy.uee.ay.\n",
      "Text  4   Sample  18  :  qurat.coiz.heenst.l..btt.nk.\n",
      "Text  4   Sample  19  :  qutat..arz.o.ldut.af...e.tt.\n",
      "Text  4   Sample  20  :  quist.trrz.seatnt.ho.set.ho.\n"
     ]
    }
   ],
   "source": [
    "def findUnknownIndex(st):\n",
    "    unknown=list()\n",
    "    known = list()\n",
    "    for i in range(0,len(st)):\n",
    "        if st[i] is '_' or st[i] is '?' :\n",
    "            unknown.append(i)\n",
    "        else:\n",
    "            known.append(i)\n",
    "    return unknown,known\n",
    "def findNeighbors(k,st):\n",
    "    i=1\n",
    "    sn=None\n",
    "    bn=None\n",
    "    while st[k-i]:\n",
    "        if st[k-i] is not '_' and st[i] is not '?':\n",
    "            sn=k-i\n",
    "            break\n",
    "        i=i+1\n",
    "    i=1\n",
    "    try:\n",
    "        while st[k+i]:\n",
    "            if st[k+i] is not '_' and st[i] is not '?':\n",
    "                bn=k+i\n",
    "                break\n",
    "            i=i+1\n",
    "    except:\n",
    "        bn = None\n",
    "    return sn,bn\n",
    "\n",
    "def predictMissingLetters(st,sample=False):\n",
    "    probMatrix=Normalize(T)\n",
    "    st = '.'+st\n",
    "    unknownIndexes,knownIndexes = findUnknownIndex(st)\n",
    "    p = list()\n",
    "    missingValues=dict()\n",
    "    for k in unknownIndexes:\n",
    "        sn,bn=findNeighbors(k,st)\n",
    "        if sn is not None and bn is not None:\n",
    "            forward = np.vstack(probMatrix).astype(float)\n",
    "            backward = np.vstack(probMatrix).astype(float)\n",
    "            normalizer = np.vstack(probMatrix).astype(float)\n",
    "            for i in range(1,k-sn):\n",
    "                forward=np.dot(forward,np.vstack(probMatrix).astype(float))\n",
    "            for i in range(1,bn-k):\n",
    "                backward=np.dot(backward,np.vstack(probMatrix).astype(float))\n",
    "            for i in range(1,bn-sn):\n",
    "                normalizer=np.dot(normalizer,np.vstack(probMatrix).astype(float))\n",
    "            bm = backward[:,letter2idx[st[bn]]]\n",
    "            fm = forward[letter2idx[st[sn]],:]\n",
    "            nm = normalizer[letter2idx[st[sn]]][letter2idx[st[bn]]]\n",
    "            predictionProb = np.multiply(bm,fm)/nm\n",
    "            missingValues[k] = predictionProb\n",
    "        else:\n",
    "            forward = np.vstack(probMatrix).astype(float)\n",
    "            for i in range(1,k-sn):\n",
    "                forward=np.dot(forward,np.vstack(probMatrix).astype(float))\n",
    "            fm = forward[letter2idx[st[sn]],:]\n",
    "            missingValues[k] = fm\n",
    "    if sample:\n",
    "        st=list(st)\n",
    "        for i in missingValues.keys():\n",
    "            st[i]=alphabet[np.random.choice(np.arange(0, 27), p=missingValues[i])]\n",
    "        st=\"\".join(st)\n",
    "    else:\n",
    "        st=list(st)\n",
    "        for i in missingValues.keys():\n",
    "            st[i]=alphabet[missingValues[i].tolist().index(max(missingValues[i]))]\n",
    "            p.append(max(missingValues[i]))\n",
    "        st=\"\".join(st)\n",
    "    total = 1\n",
    "    for x in p:\n",
    "        total *= x   \n",
    "    return st[1:len(st)],math.log(total)\n",
    "\n",
    "def sampleMissingLetters(st,Ntrial):\n",
    "    for k in st:\n",
    "        for trial in range(0,Ntrial):\n",
    "            print 'Text ',st.index(k)+1,\" \",'Sample ',trial+1,\" : \",predictMissingLetters(k,sample=True)[0]\n",
    "            \n",
    "def predictMostLikelyMissingLetters(st):\n",
    "    for k in st:\n",
    "        txt,probs=predictMissingLetters(k,sample=False)\n",
    "        print 'String: ',txt\n",
    "        print 'Log-Probability: '\n",
    "        print probs\n",
    "\n",
    "sampleMissingLetters(test_strings,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Fill the Missing Letters By Most Likely Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  the.br.an.fex.\n",
      "Log-Probability: \n",
      "-3.5838206068\n",
      "String:  oursthand.to.be.answere.\n",
      "Log-Probability: \n",
      "-11.4410973481\n",
      "String:  in.ath.wathend.he.r.ing\n",
      "Log-Probability: \n",
      "-11.8189779975\n",
      "String:  qur.t.terz.t....t.ae.t.e.ae.\n",
      "Log-Probability: \n",
      "-27.5276806691\n"
     ]
    }
   ],
   "source": [
    "predictMostLikelyMissingLetters(test_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to Improve the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this model we are using Markov(1) model, this means that we are considering only the closest given letters from both sides. Although it is a very simple approach and efficient to calculate, a letter in a word is also related to the other words in a model. Sequencing of letters in a word is very important feature that should be considered. Therefore using higher order Markov model possibly give better results.   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
